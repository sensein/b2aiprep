{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import IPython.display as Ipd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "#b2aiprep is a library with various functions to load and process your files\n",
    "import b2aiprep.process as b2p\n",
    "import b2aiprep.demographics as b2dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full participant information file\n",
    "data_path = Path.cwd().joinpath('bridge2ai-Voice', 'bridge2ai-voice-corpus-1')\n",
    "# data_path = Path.home().joinpath('data', 'bridge2ai', 'bridge2ai-voice-corpus-1')\n",
    "\n",
    "df = b2dm.load_csv_file(data_path / 'bridge2ai_voice_data.csv')\n",
    "\n",
    "#create separate data frames for sets of columns\n",
    "#number of participants\n",
    "participants_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.PARTICIPANT)\n",
    "print('Number of participants:', len(participants_df))\n",
    "\n",
    "# session info\n",
    "sessions_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.SESSION)\n",
    "print('Number of sessions:', len(sessions_df))\n",
    "\n",
    "#subject id (record_id) to acoustic_task_id and acoustic_task_name \n",
    "acoustic_tasks_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.ACOUSTIC_TASK)\n",
    "print('Number of Acoustic Tasks:', len(acoustic_tasks_df))\n",
    "\n",
    "#recording info\n",
    "recordings_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.RECORDING)\n",
    "print('Number of Recordings:', len(recordings_df))\n",
    "\n",
    "#demographics\n",
    "generic_demographics_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_DEMOGRAPHICS)\n",
    "print('Number of Demographics:', len(generic_demographics_df))\n",
    "\n",
    "#confunds\n",
    "generic_confounders_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_CONFOUNDERS)\n",
    "print('Number of Confounders:', len(generic_confounders_df))\n",
    "\n",
    "#phq9 depression individual question scores\n",
    "phq9_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_PHQ9_DEPRESSION)\n",
    "print('Number of PHQ9 entries:', len(phq9_df))\n",
    "\n",
    "#gad7 anxiety individual question scores\n",
    "gad7_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_GAD7_ANXIETY)\n",
    "print('Number of GAD7 entries:', len(gad7_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the above variables is a pandas \"DataFrame\". The easiest way to preview these dataframes is to use the `.head()` method, which displays the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df['record_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how you can create one Python object which has *all* of the information for an individual patient.\n",
    "This is fairly verbose, but gives you a good idea of all the information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = []\n",
    "\n",
    "for participant in participants_df.to_dict('records'):\n",
    "    participants.append(participant)\n",
    "    participant['sessions'] = sessions_df[sessions_df['record_id'] == participant['record_id']].to_dict('records')\n",
    "    \n",
    "    for session in participant['sessions']:\n",
    "        # there can be multiple acoustic tasks per session\n",
    "        session_id = session['session_id']\n",
    "        session['acoustic_tasks'] = acoustic_tasks_df[acoustic_tasks_df['acoustic_task_session_id'] == session_id].to_dict('records')\n",
    "        for task in session['acoustic_tasks']:\n",
    "            # there can be multiple recordings per acoustic task\n",
    "            task['recordings'] = recordings_df[recordings_df['recording_acoustic_task_id'] == task['acoustic_task_id']].to_dict('records')\n",
    "        \n",
    "        # there can be only one demographics per session\n",
    "        session['generic_demographics'] = (generic_demographics_df[generic_demographics_df['demographics_session_id'] == session_id].to_dict('records')[:1] or [None])[0]\n",
    "        # there can be only one confounders per session\n",
    "        session['generic_confounders'] = (generic_confounders_df[generic_confounders_df['confounders_session_id'] == session_id].to_dict('records')[:1] or [None])[0]\n",
    "\n",
    "print(json.dumps(participants[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can look across all values for a field such as `age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at all the possible ages in the participant data frame\n",
    "participants_df['age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not all of these are numbers! '90 or older'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic tasks\n",
    "\n",
    "Let's look at the acoustic tasks dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the above corresponds to a different acoustic task: an audio check, prolonged vowels, etc. The `value_counts()` method for pandas DataFrames lets us count all the unique values for a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks_df['acoustic_task_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the rainbow passage for the first `record_id` in the dataset, and see if we can load in the corresponding audio / spectrogram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id = acoustic_tasks_df['record_id'].values[0]\n",
    "\n",
    "# create an index into the dataframe which gets the row we are interested in\n",
    "idx = (acoustic_tasks_df['record_id'] == record_id) & (acoustic_tasks_df['acoustic_task_name'] == 'Rainbow Passage')\n",
    "\n",
    "display(acoustic_tasks_df.loc[idx])\n",
    "\n",
    "# note we use .values[0] to get the first value of a length-1 array\n",
    "acoustic_task_session_id = acoustic_tasks_df.loc[idx, 'acoustic_task_session_id'].values[0]\n",
    "acoustic_task_id = acoustic_tasks_df.loc[idx, 'acoustic_task_id'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to disambiguate a few of these columns:\n",
    "\n",
    "- `record_id`: a unique identifier for each participant\n",
    "- `recording_id` a unique identifier for each audio recording (and subsequently each audio spectrogram)\n",
    "- `acoustic_task_session_id`: unique identifier for a session where they are recording an ID.\n",
    "- `acoustic_task_id`: a unique identifier for each acoustic task *for* each acoustic session.\n",
    "\n",
    "This is a great opportunity to take a look at the data dictionary which has all of this information.\n",
    "\n",
    "Now that we know the session identifier, we can acquire the `recording_id` associated with this acoustic task session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (recordings_df['recording_acoustic_task_id'] == acoustic_task_id)\n",
    "\n",
    "display(recordings_df.loc[idx])\n",
    "\n",
    "recording_id = recordings_df.loc[idx, 'recording_id'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very tedious! We've written a function to get recording IDs for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = b2dm.get_recordings_for_acoustic_task(df, acoustic_task='Rainbow Passage')\n",
    "recordings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data\n",
    "\n",
    "The audio data has been processed into a Pytorch file, with the following dictionary keys:\n",
    "\n",
    "- 'specgram'\n",
    "- 'melfilterbank'\n",
    "- 'mfcc'\n",
    "- 'opensmile'\n",
    "- 'sample_rate'\n",
    "- 'checksum'\n",
    "- 'transcription'\n",
    "\n",
    "The audio data is also in a subfolder, \"data\". Let's create this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = data_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_id = '529F2F35-ECC1-42EB-81F0-5D28E0CE4E75'\n",
    "features = torch.load(data_path.joinpath('data') / f\"{recording_id}_features.pt\")\n",
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectogram\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "b2p.plot_spectrogram(torch.log10(features['specgram'].T), ax=axs)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconstruct the audio for the spectrogram, but note that the spectrogram has been modified to protect privacy, so the reconstructed audio sounds a bit unusual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2 * (features['specgram'].shape[1] - 1)\n",
    "sr = features['sample_rate']\n",
    "win_length = int(sr * 25 / 1000)\n",
    "hop_length = int(sr * 15 / 1000)\n",
    "griffin_lim = torchaudio.transforms.GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=2)\n",
    "reconstructed_waveform = griffin_lim(features['specgram'].T)\n",
    "Ipd.display(Ipd.Audio(data=reconstructed_waveform, rate=sr*15/25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a helper function to load in all the spectograms for the above Rainbow Passage task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = b2dm.load_features_for_recordings(recordings, audio_path, 'specgram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the index below (`i = 0`) to see different spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the spectogram\n",
    "i = 0\n",
    "recording_id = list(spectrograms.keys())[i]\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "b2p.plot_spectrogram(torch.log10(spectrograms[recording_id].T), ax=axs)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (voice)",
   "language": "python",
   "name": "voice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
