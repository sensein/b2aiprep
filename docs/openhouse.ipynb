{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as Ipd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "#b2aiprep is a library with various functions to load and process your files\n",
    "import b2aiprep.process as b2p\n",
    "import b2aiprep.demographics as b2dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full participant information file\n",
    "data_path = Path.cwd().joinpath('bridge2ai-Voice', 'bridge2ai-voice-corpus-1')\n",
    "# data_path = Path.home().joinpath('data', 'bridge2ai', 'bridge2ai-voice-corpus-1')\n",
    "\n",
    "df = b2dm.load_csv_file(data_path / 'bridge2ai_voice_data.csv')\n",
    "\n",
    "#create separate data frames for sets of columns\n",
    "#number of participants\n",
    "participants_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.PARTICIPANT)\n",
    "print('Number of participants:', len(participants_df))\n",
    "\n",
    "# session info\n",
    "sessions_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.SESSION)\n",
    "print('Number of sessions:', len(sessions_df))\n",
    "\n",
    "#subject id (record_id) to acoustic_task_id and acoustic_task_name \n",
    "acoustic_tasks_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.ACOUSTIC_TASK)\n",
    "print('Number of Acoustic Tasks:', len(acoustic_tasks_df))\n",
    "\n",
    "#recording info\n",
    "recordings_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.RECORDING)\n",
    "print('Number of Recordings:', len(recordings_df))\n",
    "\n",
    "#demographics\n",
    "generic_demographics_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_DEMOGRAPHICS)\n",
    "print('Number of Demographics:', len(generic_demographics_df))\n",
    "\n",
    "#confunds\n",
    "generic_confounders_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_CONFOUNDERS)\n",
    "print('Number of Confounders:', len(generic_confounders_df))\n",
    "\n",
    "#phq9 depression individual question scores\n",
    "phq9_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_PHQ9_DEPRESSION)\n",
    "print('Number of PHQ9 entries:', len(phq9_df))\n",
    "\n",
    "#gad7 anxiety individual question scores\n",
    "gad7_df = b2dm.get_df_of_repeat_instrument(df, b2dm.RepeatInstrument.GENERIC_GAD7_ANXIETY)\n",
    "print('Number of GAD7 entries:', len(gad7_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the above variables is a pandas \"DataFrame\". The easiest way to preview these dataframes is to use the `.head()` method, which displays the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df['record_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how you can create one Python object which has *all* of the information for an individual patient.\n",
    "This is fairly verbose, but gives you a good idea of all the information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = []\n",
    "\n",
    "for participant in participants_df.to_dict('records'):\n",
    "    participants.append(participant)\n",
    "    participant['sessions'] = sessions_df[sessions_df['record_id'] == participant['record_id']].to_dict('records')\n",
    "    \n",
    "    for session in participant['sessions']:\n",
    "        # there can be multiple acoustic tasks per session\n",
    "        session_id = session['session_id']\n",
    "        session['acoustic_tasks'] = acoustic_tasks_df[acoustic_tasks_df['acoustic_task_session_id'] == session_id].to_dict('records')\n",
    "        for task in session['acoustic_tasks']:\n",
    "            # there can be multiple recordings per acoustic task\n",
    "            task['recordings'] = recordings_df[recordings_df['recording_acoustic_task_id'] == task['acoustic_task_id']].to_dict('records')\n",
    "        \n",
    "        # there can be only one demographics per session\n",
    "        session['generic_demographics'] = (generic_demographics_df[generic_demographics_df['demographics_session_id'] == session_id].to_dict('records')[:1] or [None])[0]\n",
    "        # there can be only one confounders per session\n",
    "        session['generic_confounders'] = (generic_confounders_df[generic_confounders_df['confounders_session_id'] == session_id].to_dict('records')[:1] or [None])[0]\n",
    "\n",
    "print(json.dumps(participants[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can look across all values for a field such as `age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at all the possible ages in the participant data frame\n",
    "participants_df['age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not all of these are numbers! '90 or older'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic tasks\n",
    "\n",
    "Let's look at the acoustic tasks dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the above corresponds to a different acoustic task: an audio check, prolonged vowels, etc. The `value_counts()` method for pandas DataFrames lets us count all the unique values for a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks_df['acoustic_task_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the rainbow passage for the first `record_id` in the dataset, and see if we can load in the corresponding audio / spectrogram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id = acoustic_tasks_df['record_id'].values[0]\n",
    "\n",
    "# create an index into the dataframe which gets the row we are interested in\n",
    "idx = (acoustic_tasks_df['record_id'] == record_id) & (acoustic_tasks_df['acoustic_task_name'] == 'Rainbow Passage')\n",
    "\n",
    "display(acoustic_tasks_df.loc[idx])\n",
    "\n",
    "# note we use .values[0] to get the first value of a length-1 array\n",
    "acoustic_task_session_id = acoustic_tasks_df.loc[idx, 'acoustic_task_session_id'].values[0]\n",
    "acoustic_task_id = acoustic_tasks_df.loc[idx, 'acoustic_task_id'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to disambiguate a few of these columns:\n",
    "\n",
    "- `record_id`: a unique identifier for each participant\n",
    "- `recording_id` a unique identifier for each audio recording (and subsequently each audio spectrogram)\n",
    "- `acoustic_task_session_id`: unique identifier for a session where they are recording an ID.\n",
    "- `acoustic_task_id`: a unique identifier for each acoustic task *for* each acoustic session.\n",
    "\n",
    "This is a great opportunity to take a look at the data dictionary which has all of this information.\n",
    "\n",
    "Now that we know the session identifier, we can acquire the `recording_id` associated with this acoustic task session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (recordings_df['recording_acoustic_task_id'] == acoustic_task_id)\n",
    "\n",
    "display(recordings_df.loc[idx])\n",
    "\n",
    "recording_id = recordings_df.loc[idx, 'recording_id'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very tedious! We've written a function to get recording IDs for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = b2dm.get_recordings_for_acoustic_task(df, acoustic_task='Rainbow Passage')\n",
    "recordings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data\n",
    "\n",
    "The audio data has been processed into a Pytorch file, with the following dictionary keys:\n",
    "\n",
    "- 'specgram'\n",
    "- 'melfilterbank'\n",
    "- 'mfcc'\n",
    "- 'opensmile'\n",
    "- 'sample_rate'\n",
    "- 'checksum'\n",
    "- 'transcription'\n",
    "\n",
    "The audio data is also in a subfolder, \"data\". Let's create this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = data_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_id = '529F2F35-ECC1-42EB-81F0-5D28E0CE4E75'\n",
    "features = torch.load(data_path.joinpath('data') / f\"{recording_id}_features.pt\", weights_only=False)\n",
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spectogram\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "b2p.plot_spectrogram(torch.log10(features['specgram'].T), ax=axs)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconstruct the audio for the spectrogram, but note that the spectrogram has been modified to protect privacy, so the reconstructed audio sounds a bit unusual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2 * (features['specgram'].shape[1] - 1)\n",
    "sr = features['sample_rate']\n",
    "win_length = int(sr * 25 / 1000)\n",
    "hop_length = int(sr * 15 / 1000)\n",
    "griffin_lim = torchaudio.transforms.GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=2)\n",
    "reconstructed_waveform = griffin_lim(features['specgram'].T)\n",
    "Ipd.display(Ipd.Audio(data=reconstructed_waveform, rate=sr*15/25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a helper function to load in all the spectograms for the above Rainbow Passage task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = b2dm.load_features_for_recordings(recordings, audio_path, 'specgram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the index below (`i = 0`) to see different spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the spectogram\n",
    "i = 0\n",
    "recording_id = list(spectrograms.keys())[i]\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "b2p.plot_spectrogram(torch.log10(spectrograms[recording_id].T), ax=axs)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
