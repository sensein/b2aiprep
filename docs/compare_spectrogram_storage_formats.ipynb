{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import shutil\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyarrow as pa\n",
    "from pyarrow.parquet import SortingColumn\n",
    "from librosa import amplitude_to_db\n",
    "\n",
    "from b2aiprep.prepare.bids import get_audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the demo dataset\n",
    "demo_subjects = [\n",
    "    '2b0751d5-10a9-46b8-9456-860e3a231588',\n",
    "    '2b1c662a-59e0-4b69-9161-a2fd4636890b',\n",
    "    '2c95136b-0efb-4d67-8fdf-113a8af5ac12',\n",
    "    '2cb188fe-4e19-4da1-a1f4-1f9f254fd1fd',\n",
    "    '2de86669-cc1f-45ef-9d31-a7eabd68f247',\n",
    "    '2e59cc45-df19-4e2d-a1d8-60c46cf47a63',\n",
    "    '04f6473c-99f1-4b65-bfe3-fcd9fa06dfc9',\n",
    "    '0b8dcad5-a2a3-4373-bdca-7289b3cbadc7',\n",
    "    '0e2df8b3-a93f-4982-a82c-d96a5c64d153',\n",
    "    '0ee1e1e1-0e86-42cc-9e9d-2cafd9f1e01c',\n",
    "    '1a7a86df-e379-40ab-a644-9821aac7be63',\n",
    "    '1a7d3f7f-714c-4727-ab83-1b400fab2070',\n",
    "    '1b07b18b-26f9-405b-a466-29442306a7fe',\n",
    "    '1c2f13bb-909b-422d-ab61-e3b2ae203ed3',\n",
    "    '1ce2db0c-505b-4bab-8cfd-04f1d5e08c4d',\n",
    "    '1f9475bb-f13b-4f68-969b-28f20455b3e7',\n",
    "    '1fcc920c-8038-4b75-8749-95bb4e4fff65',\n",
    "    '2a86baba-6c34-4c34-9f7e-1d49d1605f6a',\n",
    "    '2e71bcdb-b388-4901-8c7d-2a3afeb80fc1',\n",
    "    '2f58e101-759a-45d4-85ff-0edf53638e52',\n",
    "    '2f9f315d-70df-4a3e-992a-c224f6cca6c6',\n",
    "    '3a744c68-53bc-438b-b5c8-7525150504d9',\n",
    "    '3b7ace99-35f8-4245-a4ce-fab50358b2c4',\n",
    "    '3bbc69ef-babd-499e-add2-1c9376ed62c6',\n",
    "    '3e55cedf-26a8-41f8-9704-12160fa43305',\n",
    "    '3ebb00fa-2e73-48b0-93e2-2ead5f573342',\n",
    "    '4af9f960-2f33-4486-b4ff-3941f6440973',\n",
    "    '4c55b481-e8a5-41fe-94c3-61d64f7a0ae2',\n",
    "    '4ce6a1af-c123-41b7-a438-c58a41d1a3d7',\n",
    "    '5c60197c-97dc-403d-bc93-4f1dbe9526f1',\n",
    "]\n",
    "\n",
    "original_path = Path.home().joinpath('data', 'bridge2ai', 'release-1.0')\n",
    "data_path = Path.home().joinpath('data', 'bridge2ai', 'release-1.0-demo')\n",
    "\n",
    "# first copy over folders for each subject\n",
    "for subject in tqdm(demo_subjects, desc='Copying subject folders'):\n",
    "    subject_folder = f'sub-{subject}'\n",
    "    shutil.copytree(original_path.joinpath(subject_folder), data_path.joinpath(subject_folder), dirs_exist_ok=True)\n",
    "\n",
    "# now load in each data file and filter\n",
    "df = pd.read_csv(original_path.joinpath('participants.tsv'), sep='\\t')\n",
    "df = df[df['record_id'].isin(demo_subjects)]\n",
    "df.to_csv(data_path.joinpath('participants.tsv'), sep='\\t', index=False)\n",
    "\n",
    "# all phenotype files in the phenotype subfolder\n",
    "for data_filepath in original_path.joinpath('phenotype').glob('*.tsv'):\n",
    "    df = pd.read_csv(data_filepath, sep='\\t')\n",
    "    df = df[df['record_id'].isin(demo_subjects)]\n",
    "    df.to_csv(data_path.joinpath('phenotype', Path(data_filepath).name), sep='\\t', index=False)\n",
    "\n",
    "    # copy over corresponding json file\n",
    "    json_filepath = data_filepath.with_suffix('.json')\n",
    "    shutil.copy(json_filepath, data_path.joinpath('phenotype', json_filepath.name))\n",
    "\n",
    "# copy over individual json files directly\n",
    "for json_filename in ['dataset_description.json', 'participants.json']:\n",
    "    shutil.copy(original_path.joinpath(json_filename), data_path.joinpath(json_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_path = Path.home().joinpath('data', 'bridge2ai', 'release-1.0-demo')\n",
    "output_base_path = Path.home().joinpath('data', 'bridge2ai', 'release-1.0-demo-processed')\n",
    "audio_paths = get_audio_paths(bids_path)\n",
    "# for debugging, subselect to those with a prefix of 0\n",
    "audio_paths = [d for d in audio_paths if d['path'].name.startswith('sub-0')]\n",
    "print(f'Found {len(audio_paths)} audio files')\n",
    "audio_paths[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load all the data, save into a pytorch file\n",
    "spectrograms = {}\n",
    "for audio_info in audio_paths:\n",
    "    pt_file = audio_info['path'].with_suffix('.pt')\n",
    "    pt_file = pt_file.with_stem(pt_file.stem + '_features')\n",
    "    data = torch.load(pt_file, weights_only=False)\n",
    "    spectrograms[audio_info['path'].stem] = data['torchaudio']['spectrogram']\n",
    "\n",
    "output_path = output_base_path.joinpath('option_1')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(spectrograms, output_path / 'spectrograms.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save as parquet\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def write_spectrograms(spec_dict, output_file):\n",
    "    data = {\n",
    "        'id': list(spec_dict.keys()),\n",
    "        'shape': [[s.shape[1]] for s in spec_dict.values()],\n",
    "        'data': [s.flatten().tolist() for s in spec_dict.values()]\n",
    "    }\n",
    "    \n",
    "    table = pa.Table.from_pydict({\n",
    "        'id': data['id'],\n",
    "        'shape': data['shape'],\n",
    "        'data': data['data']\n",
    "    })\n",
    "    \n",
    "    pq.write_table(table, output_file, compression='zstd', compression_level=3)\n",
    "\n",
    "def read_spectrograms(input_file):\n",
    "    table = pq.read_table(input_file)\n",
    "    spec_dict = {}\n",
    "    \n",
    "    for i in range(len(table)):\n",
    "        id_ = table['id'][i].as_py()\n",
    "        shape = table['shape'][i][0].as_py()\n",
    "        data = np.array(table['data'][i].as_py()).reshape(513, shape)\n",
    "        spec_dict[id_] = data\n",
    "    \n",
    "    return spec_dict\n",
    "\n",
    "output_path = output_base_path.joinpath('option_2')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "write_spectrograms(spectrograms, output_path / 'spectrograms.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: HF dataset\n",
    "\n",
    "def audio_feature_generator(\n",
    "    audio_paths,\n",
    ") -> t.Generator[t.Dict[str, t.Any], None, None]:\n",
    "    \"\"\"Load audio features from individual files and yield dictionaries amenable to HuggingFace's Dataset from_generator.\"\"\"\n",
    "    for wav_path in tqdm(audio_paths, total=len(audio_paths), desc=\"Extracting features\"):\n",
    "        output = {}\n",
    "        pt_file = wav_path.parent / f\"{wav_path.stem}_features.pt\"\n",
    "        features = torch.load(pt_file, weights_only=False)\n",
    "        output['spectrogram'] = features['torchaudio']['spectrogram'].numpy().astype(np.float32)\n",
    "\n",
    "        yield output\n",
    "\n",
    "audio_paths = [Path(d['path']) for d in audio_paths]\n",
    "audio_feature_generator_partial = partial(audio_feature_generator, audio_paths=audio_paths)\n",
    "ds = Dataset.from_generator(audio_feature_generator_partial, num_proc=1)\n",
    "\n",
    "output_path = output_base_path.joinpath('option_3')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    # pyarrow Parquet writer options\n",
    "    compression=\"snappy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4: Save each spectrogram as a separate file\n",
    "output_path = output_base_path.joinpath('option_4')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "for spec in spectrograms:\n",
    "    torch.save(spectrograms[spec], output_path.joinpath(f'{spec}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 5: Use a pandas dataframe intermediary and partitioning\n",
    "# TODO: this doesn't work currently\n",
    "# dataset = Dataset.from_pandas(df, preserve_index=True)\n",
    "#   > ArrowInvalid: ('Can only convert 1-dimensional array values', 'Conversion failed for column array with type object')\n",
    "\n",
    "def create_partitioned_dataset(\n",
    "        spectrograms: t.Dict[str, np.ndarray],\n",
    "        # tasks: t.List[str], \n",
    "        save_dir: str, partition_size: int = 1000\n",
    "):\n",
    "    # Extract metadata\n",
    "    ids = list(spectrograms.keys())\n",
    "    arrays = [x.numpy() for x in spectrograms.values()]\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        # 'task': tasks,\n",
    "        'array_length': [arr.shape[1] for arr in arrays],\n",
    "        'array': arrays\n",
    "    })\n",
    "    \n",
    "    # Create multi-index for efficient filtering\n",
    "    # df.set_index(['id', 'task'], inplace=True)\n",
    "    df.set_index(['id'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Convert to HF dataset\n",
    "    dataset = Dataset.from_pandas(df, preserve_index=True)\n",
    "    \n",
    "    # Save with automatic partitioning\n",
    "    dataset.save_to_disk(\n",
    "        save_dir,\n",
    "        num_shards=max(1, len(df) // partition_size),\n",
    "        max_shard_size=\"500MB\"\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_partitioned_dataset(save_dir: str):\n",
    "    return Dataset.load_from_disk(save_dir)\n",
    "\n",
    "\n",
    "output_path = output_base_path.joinpath('option_5')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "# dataset = create_partitioned_dataset(\n",
    "#     spectrograms=spectrograms,\n",
    "#     # tasks=tasks,\n",
    "#     save_dir=str(output_path),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 7: the above was bigger so we use the generator from option 3\n",
    "def audio_feature_generator_sorted(\n",
    "    audio_paths,\n",
    ") -> t.Generator[t.Dict[str, t.Any], None, None]:\n",
    "    \"\"\"Load audio features from individual files and yield dictionaries amenable to HuggingFace's Dataset from_generator.\"\"\"\n",
    "    audio_paths = sorted(\n",
    "        audio_paths,\n",
    "        # sort first by subject, then by task\n",
    "        key=lambda x: (x.stem.split('_')[0], x.stem.split('_')[2])\n",
    "    )\n",
    "\n",
    "    for wav_path in tqdm(audio_paths, total=len(audio_paths), desc=\"Extracting features\"):\n",
    "        output = {}\n",
    "        pt_file = wav_path.parent / f\"{wav_path.stem}_features.pt\"\n",
    "        features = torch.load(pt_file, weights_only=False)\n",
    "\n",
    "        \n",
    "        output['record_id'] = wav_path.stem.split('_')[0][4:] # skip \"sub-\" prefix\n",
    "        output['task_name'] = wav_path.stem.split('_')[2][5:] # skip \"task-\" prefix\n",
    "        output['spectrogram'] = features['torchaudio']['spectrogram'].numpy().astype(np.float32)\n",
    "\n",
    "        yield output\n",
    "\n",
    "audio_feature_generator_sorted_partial = partial(\n",
    "    audio_feature_generator_sorted,\n",
    "    audio_paths=audio_paths,\n",
    ")\n",
    "\n",
    "# sort the dataset by identifier and task_name\n",
    "ds = Dataset.from_generator(audio_feature_generator_sorted_partial, num_proc=1)\n",
    "sorting_columns = [\n",
    "    SortingColumn(column_index=0, descending=False),\n",
    "    SortingColumn(column_index=1, descending=False),\n",
    "]\n",
    "\n",
    "output_path = output_base_path.joinpath('option_7')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# default options\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"snappy\",\n",
    "    use_dictionary=[\"record_id\", \"task_name\"],\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,\n",
    "    write_page_index=True,\n",
    "    sorting_columns=sorting_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 8: try to compress with zstd level 9\n",
    "output_path = output_base_path.joinpath('option_8')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",\n",
    "    compression_level=9,\n",
    "    use_dictionary=[\"record_id\", \"task_name\"],\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,\n",
    "    write_page_index=True,\n",
    "    sorting_columns=sorting_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 9: try to compress with zstd level 3\n",
    "output_path = output_base_path.joinpath('option_9')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",\n",
    "    compression_level=3,\n",
    "    use_dictionary=[\"record_id\", \"task_name\"],\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,\n",
    "    write_page_index=True,\n",
    "    sorting_columns=sorting_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zstd > snappy.\n",
    "\n",
    "what about pivoting the columns so that columnar compression is done for each frequency independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 10: transpose the spectrograms for better column compression efficiency maybe?\n",
    "def audio_feature_generator_sorted(\n",
    "    audio_paths,\n",
    ") -> t.Generator[t.Dict[str, t.Any], None, None]:\n",
    "    \"\"\"Load audio features from individual files and yield dictionaries amenable to HuggingFace's Dataset from_generator.\"\"\"\n",
    "    audio_paths = sorted(\n",
    "        audio_paths,\n",
    "        # sort first by subject, then by task\n",
    "        key=lambda x: (x.stem.split('_')[0], x.stem.split('_')[2])\n",
    "    )\n",
    "\n",
    "    for wav_path in tqdm(audio_paths, total=len(audio_paths), desc=\"Extracting features\"):\n",
    "        output = {}\n",
    "        pt_file = wav_path.parent / f\"{wav_path.stem}_features.pt\"\n",
    "        features = torch.load(pt_file, weights_only=False)\n",
    "\n",
    "        \n",
    "        output['record_id'] = wav_path.stem.split('_')[0][4:] # skip \"sub-\" prefix\n",
    "        output['task_name'] = wav_path.stem.split('_')[2][5:] # skip \"task-\" prefix\n",
    "        # transpose the spectrogram\n",
    "        output['spectrogram'] = features['torchaudio']['spectrogram'].numpy().astype(np.float32).T\n",
    "\n",
    "        yield output\n",
    "\n",
    "audio_feature_generator_sorted_partial = partial(\n",
    "    audio_feature_generator_sorted,\n",
    "    audio_paths=audio_paths,\n",
    ")\n",
    "\n",
    "# sort the dataset by identifier and task_name\n",
    "ds = Dataset.from_generator(audio_feature_generator_sorted_partial, num_proc=1)\n",
    "\n",
    "output_path = output_base_path.joinpath('option_10')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",  # Better compression ratio than snappy, still good speed\n",
    "    compression_level=9,\n",
    "    use_dictionary=[\"record_id\", \"task_name\"],  # Enable dictionary encoding for strings\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,  # 1MB pages\n",
    "    write_page_index=True,  # Enable page index for better filtering\n",
    "    sorting_columns=(\n",
    "        SortingColumn(column_index=0, descending=False),\n",
    "        SortingColumn(column_index=1, descending=False),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nope! it compresses better when the individual time samples are the columns.\n",
    "\n",
    "OK, back to option 7, but let's try to convert the spectrogram to power, and add session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_feature_power(\n",
    "    audio_paths,\n",
    ") -> t.Generator[t.Dict[str, t.Any], None, None]:\n",
    "    \"\"\"Load audio features from individual files and yield dictionaries amenable to HuggingFace's Dataset from_generator.\"\"\"\n",
    "    audio_paths = sorted(\n",
    "        audio_paths,\n",
    "        # sort first by subject, then by task\n",
    "        key=lambda x: (x.stem.split('_')[0], x.stem.split('_')[2])\n",
    "    )\n",
    "\n",
    "    for wav_path in tqdm(audio_paths, total=len(audio_paths), desc=\"Extracting features\"):\n",
    "        output = {}\n",
    "        pt_file = wav_path.parent / f\"{wav_path.stem}_features.pt\"\n",
    "        features = torch.load(pt_file, weights_only=False)\n",
    "\n",
    "        \n",
    "        output['participant_id'] = wav_path.stem.split('_')[0][4:] # skip \"sub-\" prefix\n",
    "        output['task_name'] = wav_path.stem.split('_')[2][5:] # skip \"task-\" prefix\n",
    "        # transpose the spectrogram\n",
    "        spectrogram = amplitude_to_db(features['torchaudio']['spectrogram'].numpy().astype(np.float32))\n",
    "        output['spectrogram'] =  spectrogram\n",
    "\n",
    "        yield output\n",
    "\n",
    "audio_feature_power_partial = partial(\n",
    "    audio_feature_power,\n",
    "    audio_paths=audio_paths,\n",
    ")\n",
    "\n",
    "# sort the dataset by identifier and task_name\n",
    "ds = Dataset.from_generator(\n",
    "    audio_feature_power_partial, num_proc=1\n",
    ")\n",
    "\n",
    "output_path = output_base_path.joinpath('option_11')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",  # Better compression ratio than snappy, still good speed\n",
    "    compression_level=3,\n",
    "    use_dictionary=[\"participant_id\", \"task_name\"],  # Enable dictionary encoding for strings\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,  # 1MB pages\n",
    "    write_page_index=True,  # Enable page index for better filtering\n",
    "    sorting_columns=(\n",
    "        SortingColumn(column_index=0, descending=False),\n",
    "        SortingColumn(column_index=1, descending=False),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked well, we can add session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_feature_power(\n",
    "    audio_paths,\n",
    ") -> t.Generator[t.Dict[str, t.Any], None, None]:\n",
    "    \"\"\"Load audio features from individual files and yield dictionaries amenable to HuggingFace's Dataset from_generator.\"\"\"\n",
    "    audio_paths = sorted(\n",
    "        audio_paths,\n",
    "        # sort first by subject, then by task\n",
    "        key=lambda x: (x.stem.split('_')[0], x.stem.split('_')[2])\n",
    "    )\n",
    "\n",
    "    for wav_path in tqdm(audio_paths, total=len(audio_paths), desc=\"Extracting features\"):\n",
    "        output = {}\n",
    "        pt_file = wav_path.parent / f\"{wav_path.stem}_features.pt\"\n",
    "        features = torch.load(pt_file, weights_only=False)\n",
    "\n",
    "        \n",
    "        output['participant_id'] = wav_path.stem.split('_')[0][4:] # skip \"sub-\" prefix\n",
    "        output['session_id'] = wav_path.stem.split('_')[1][4:] # skip \"ses-\" prefix\n",
    "        output['task_name'] = wav_path.stem.split('_')[2][5:] # skip \"task-\" prefix\n",
    "        # transpose the spectrogram\n",
    "        spectrogram = amplitude_to_db(features['torchaudio']['spectrogram'].numpy().astype(np.float32))\n",
    "        output['spectrogram'] =  spectrogram\n",
    "\n",
    "        yield output\n",
    "\n",
    "audio_feature_power_partial = partial(\n",
    "    audio_feature_power,\n",
    "    audio_paths=audio_paths,\n",
    ")\n",
    "\n",
    "# sort the dataset by identifier and task_name\n",
    "ds = Dataset.from_generator(\n",
    "    audio_feature_power_partial, num_proc=1\n",
    ")\n",
    "\n",
    "output_path = output_base_path.joinpath('option_12')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",  # Better compression ratio than snappy, still good speed\n",
    "    compression_level=3,\n",
    "    use_dictionary=[\"participant_id\", \"session_id\", \"task_name\"],  # Enable dictionary encoding for strings\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,  # 1MB pages\n",
    "    write_page_index=True,  # Enable page index for better filtering\n",
    "    sorting_columns=(\n",
    "        SortingColumn(column_index=0, descending=False),\n",
    "        SortingColumn(column_index=2, descending=False),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That compresses better. Try converting to decimal and quantizing to see if we can do some clever integer compression of decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_feature_power_decimal(audio_paths) -> t.Generator[t.Dict[str, t.Any], None, None]:\n",
    "    audio_paths = sorted(audio_paths, key=lambda x: (x.stem.split('_')[0], x.stem.split('_')[2]))\n",
    "    \n",
    "    for wav_path in tqdm(audio_paths, total=len(audio_paths), desc=\"Extracting features\"):\n",
    "        output = {}\n",
    "        pt_file = wav_path.parent / f\"{wav_path.stem}_features.pt\"\n",
    "        features = torch.load(pt_file, weights_only=False)\n",
    "        \n",
    "        output['participant_id'] = wav_path.stem.split('_')[0][4:]\n",
    "        output['session_id'] = wav_path.stem.split('_')[1][4:]\n",
    "        output['task_name'] = wav_path.stem.split('_')[2][5:]\n",
    "        \n",
    "        # Process spectrogram\n",
    "        spec = amplitude_to_db(features['torchaudio']['spectrogram'].numpy().astype(np.float32))\n",
    "        n_row, n_col = spec.shape\n",
    "        # Round to 4 decimal places\n",
    "        spec_rounded = np.round(spec, decimals=4)\n",
    "        # cast to decimal type\n",
    "        spec_decimal = pa.array(spec_rounded.flatten(), type=pa.decimal128(8, 4))\n",
    "        # Reshape back to original dimensions\n",
    "        output['spectrogram'] = spec_decimal.reshape(n_row, n_col)\n",
    "        \n",
    "        yield output\n",
    "\n",
    "ds = Dataset.from_generator(\n",
    "    partial(\n",
    "        audio_feature_power_decimal,\n",
    "        audio_paths=audio_paths,\n",
    "    ), num_proc=1)\n",
    "\n",
    "output_path = output_base_path.joinpath('option_13')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",\n",
    "    compression_level=3,\n",
    "    use_dictionary=[\"participant_id\", \"session_id\", \"task_name\"],\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,\n",
    "    write_page_index=True,\n",
    "    sorting_columns=(\n",
    "        SortingColumn(column_index=0, descending=False),\n",
    "        SortingColumn(column_index=2, descending=False),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with the full dataset\n",
    "\n",
    "- Power spectrum in dB\n",
    "- Not using decimal packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_audio_paths = get_audio_paths(Path.home().joinpath('data', 'bridge2ai', 'release-1.0'))\n",
    "# for debugging, subselect to those with a prefix of 0\n",
    "full_dataset_audio_paths = [d['path'] for d in full_dataset_audio_paths]\n",
    "print(f'Found {len(full_dataset_audio_paths)} audio files.')\n",
    "audio_feature_power_partial = partial(\n",
    "    audio_feature_power,\n",
    "    audio_paths=full_dataset_audio_paths,\n",
    ")\n",
    "\n",
    "# sort the dataset by identifier and task_name\n",
    "ds = Dataset.from_generator(\n",
    "    partial(\n",
    "        audio_feature_power,\n",
    "        audio_paths=full_dataset_audio_paths,\n",
    "    ),\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "output_path = output_base_path.joinpath('option_12_full')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds.to_parquet(\n",
    "    str(output_path / \"spectrograms.parquet\"),\n",
    "    compression=\"zstd\",  # Better compression ratio than snappy, still good speed\n",
    "    compression_level=3,\n",
    "    use_dictionary=[\"participant_id\", \"session_id\", \"task_name\"],  # Enable dictionary encoding for strings\n",
    "    write_statistics=True,\n",
    "    data_page_size=1_048_576,  # 1MB pages\n",
    "    write_page_index=True,  # Enable page index for better filtering\n",
    "    sorting_columns=(\n",
    "        SortingColumn(column_index=0, descending=False),\n",
    "        SortingColumn(column_index=2, descending=False),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice loading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_parquet(\n",
    "    output_base_path.joinpath('option_7', 'spectrograms.parquet').as_posix(),\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "# plot a single spectrogram\n",
    "idx = 18\n",
    "print(dataset[idx]['record_id'])\n",
    "print(dataset[idx]['task_name'])\n",
    "spectrogram = np.asarray(dataset[idx]['spectrogram'])\n",
    "\n",
    "# transform to decibel\n",
    "spectrogram = amplitude_to_db(spectrogram)\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.imshow(spectrogram[:,:], aspect='auto', origin='lower')\n",
    "# set x-ticks to be time in seconds, where each sample is 10ms\n",
    "plt.xticks(\n",
    "    np.linspace(0, spectrogram.shape[1], 11),\n",
    "    np.linspace(0, spectrogram.shape[1] / 100, 11).astype(int)\n",
    ")\n",
    "plt.xlabel('Time (s)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_parquet(\n",
    "    output_base_path.joinpath('option_12', 'spectrograms.parquet').as_posix(),\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "idx = 18\n",
    "print(dataset[idx]['record_id'])\n",
    "print(dataset[idx]['task_name'])\n",
    "spectrogram = np.asarray(dataset[idx]['spectrogram'])\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.imshow(spectrogram, aspect='auto', origin='lower')\n",
    "# set x-ticks to be time in seconds, where each sample is 10ms\n",
    "plt.xticks(\n",
    "    np.linspace(0, spectrogram.shape[1], 11),\n",
    "    np.linspace(0, spectrogram.shape[1] / 100, 11).astype(int)\n",
    ")\n",
    "plt.xlabel('Time (s)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as Ipd\n",
    "import torchaudio\n",
    "from librosa import db_to_amplitude\n",
    "spectrogram = dataset[idx]['spectrogram']\n",
    "# reconstruct the waveform\n",
    "spectrogram = db_to_amplitude(np.asarray(spectrogram))\n",
    "n_fft = 2 * (spectrogram.shape[0] - 1)\n",
    "sr = 16000\n",
    "win_length = int(sr * 25 / 1000)\n",
    "hop_length = int(sr * 10 / 1000)\n",
    "griffin_lim = torchaudio.transforms.GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=2)\n",
    "reconstructed_waveform = griffin_lim(torch.tensor(spectrogram))\n",
    "Ipd.display(Ipd.Audio(data=reconstructed_waveform, rate=sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
